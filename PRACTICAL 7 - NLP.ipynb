{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf48355",
   "metadata": {},
   "source": [
    "1.\tWrite a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for 1 billion dollar.\"\n",
    "\n",
    "2.\tUsing NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence.\n",
    "\n",
    "3.\tWrite a Python program that uses NLTK to extract and display all noun phrases from a given text.\n",
    "\n",
    "4.\tUsing NLTK, write a program to perform chunking on the sentence: \"He reckons the current account deficit will narrow to only     8 billion.\" and display the chunked tree.\n",
    "\n",
    "5.\tWrite a Python function using NLTK that takes a sentence as input and returns all verb phrases (VP) present in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5e6a9",
   "metadata": {},
   "source": [
    "1.\tWrite a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for 1 billion dollar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bea658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Inc.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_named_entities(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "    tree = ne_chunk(tagged)\n",
    "    \n",
    "    named_entities = []\n",
    "    for subtree in tree:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            entity = \" \".join([word for word, tag in subtree])\n",
    "            named_entities.append(entity)\n",
    "    \n",
    "    return named_entities\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
    "print(extract_named_entities(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a1103",
   "metadata": {},
   "source": [
    "2.\tUsing NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe51ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "Entity: Apple, Type: PERSON, POS Tags: [('Apple', 'NNP')]\n",
      "Entity: Inc., Type: ORGANIZATION, POS Tags: [('Inc.', 'NNP')]\n",
      "\n",
      "Sentence 2:\n",
      "Entity: Elon, Type: PERSON, POS Tags: [('Elon', 'NNP')]\n",
      "Entity: Musk, Type: PERSON, POS Tags: [('Musk', 'NNP')]\n",
      "Entity: SpaceX, Type: ORGANIZATION, POS Tags: [('SpaceX', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_named_entities_with_pos(sentences):\n",
    "    named_entities_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        ne_tree = ne_chunk(pos_tags)\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for subtree in ne_tree:\n",
    "            if isinstance(subtree, nltk.Tree):  # Named entity subtree\n",
    "                entity_name = \" \".join([word for word, tag in subtree])\n",
    "                entity_type = subtree.label()\n",
    "                pos_info = [(word, tag) for word, tag in subtree]\n",
    "                entities.append({\n",
    "                    'entity': entity_name,\n",
    "                    'type': entity_type,\n",
    "                    'pos_tags': pos_info\n",
    "                })\n",
    "\n",
    "        named_entities_list.append(entities)\n",
    "\n",
    "    return named_entities_list\n",
    "\n",
    "# Example usage\n",
    "sentences = [\n",
    "    \"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
    "    \"Elon Musk founded SpaceX in 2002.\"\n",
    "]\n",
    "\n",
    "result = extract_named_entities_with_pos(sentences)\n",
    "\n",
    "# Display results\n",
    "for i, ents in enumerate(result):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    for ent in ents:\n",
    "        print(f\"Entity: {ent['entity']}, Type: {ent['type']}, POS Tags: {ent['pos_tags']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68096d1c",
   "metadata": {},
   "source": [
    "3.\tWrite a Python program that uses NLTK to extract and display all noun phrases from a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c198d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox', 'the lazy dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_noun_phrases(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    \n",
    "    # Define the grammar for noun phrases\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "    parser = RegexpParser(grammar)\n",
    "    \n",
    "    tree = parser.parse(tagged)\n",
    "    \n",
    "    noun_phrases = []\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        noun_phrases.append(\" \".join(word for word, tag in subtree))\n",
    "    \n",
    "    return noun_phrases\n",
    "\n",
    "# Example usage:\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "print(extract_noun_phrases(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa23b1",
   "metadata": {},
   "source": [
    "4.\tUsing NLTK, write a program to perform chunking on the sentence: \"He reckons the current account deficit will narrow to only 8 billion.\" and display the chunked tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc43792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgling\n",
      "  Obtaining dependency information for svgling from https://files.pythonhosted.org/packages/87/d0/570cbaff44446824b08084c1ce4b47efca0727a5a9bf11c233177ea09b05/svgling-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting svgwrite (from svgling)\n",
      "  Obtaining dependency information for svgwrite from https://files.pythonhosted.org/packages/84/15/640e399579024a6875918839454025bb1d5f850bb70d96a11eabb644d11c/svgwrite-1.4.3-py3-none-any.whl.metadata\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
      "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "   ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 30.7/67.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 67.1/67.1 kB 902.9 kB/s eta 0:00:00\n",
      "Installing collected packages: svgwrite, svgling\n",
      "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n",
      "(S\n",
      "  He/PRP\n",
      "  reckons/VBZ\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  will/MD\n",
      "  narrow/VB\n",
      "  to/TO\n",
      "  only/RB\n",
      "  8/CD\n",
      "  billion/CD\n",
      "  ./.)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,688.0,168.0\" width=\"688px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.81395%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">He</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.90698%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4651%\" x=\"5.81395%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">reckons</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.0465%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.2093%\" x=\"16.2791%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"15.625%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">current</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.6875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"43.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">account</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"71.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">deficit</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.9375%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8837%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"53.4884%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9767%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.30233%\" x=\"60.4651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">narrow</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.1163%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"69.7674%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.093%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"74.4186%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">only</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.907%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"81.3953%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">8</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.7209%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4651%\" x=\"86.0465%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">billion</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.2791%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.48837%\" x=\"96.5116%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2558%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('He', 'PRP'), ('reckons', 'VBZ'), Tree('NP', [('the', 'DT'), ('current', 'JJ'), ('account', 'NN'), ('deficit', 'NN')]), ('will', 'MD'), ('narrow', 'VB'), ('to', 'TO'), ('only', 'RB'), ('8', 'CD'), ('billion', 'CD'), ('.', '.')])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install svgling\n",
    "\n",
    "import svgling\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from IPython.display import display # Import display\n",
    "\n",
    "sentence = \"He reckons the current account deficit will narrow to only 8 billion.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN.*>+}\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
    "    PP: {<IN><NP>}\n",
    "\"\"\"\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "chunked = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunked)\n",
    "display(chunked)  # Use display instead of chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d03fc",
   "metadata": {},
   "source": [
    "5.\tWrite a Python function using NLTK that takes a sentence as input and returns all verb phrases (VP) present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bbb72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is writing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_verb_phrases(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Improved grammar for verb phrases\n",
    "    grammar = r\"\"\"\n",
    "        VP: {<MD>?<VB.*>+<NP|PP|CLAUSE>*}\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "    chunked_tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "    verb_phrases = []\n",
    "    for subtree in chunked_tree.subtrees():\n",
    "        if subtree.label() == 'VP':\n",
    "            verb_phrases.append(\" \".join(word for word, tag in subtree.leaves()))\n",
    "\n",
    "    return verb_phrases\n",
    "\n",
    "sentence = \"She is writing a research paper on artificial intelligence.\"\n",
    "print(extract_verb_phrases(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

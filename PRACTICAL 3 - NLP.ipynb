{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e68d45",
   "metadata": {},
   "source": [
    "1.\tWrite a Python program to download the text of \"Pride and Prejudice\" by Jane Austen from Project Gutenberg, tokenize the text, and display the first 10 tokens.\n",
    "\n",
    "2.\tUsing NLTK, write a function that takes a URL as input, fetches the raw text from the webpage, and returns the number of words in the text.\n",
    "\n",
    "3.\tExplain how to remove HTML tags from a web page's content using Python and NLTK. Provide a code example that fetches a web page, removes HTML tags, and prints the cleaned text.\n",
    "\n",
    "4.\tWrite a Python program that reads a text file, tokenizes its content into sentences, and prints the number of sentences in the file\n",
    "\n",
    "5.\tUsing regular expressions in Python, write a function that takes a list of words and returns a list of words that end with 'ing'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea452580",
   "metadata": {},
   "source": [
    "1.\tWrite a Python program to download the text of \"Pride and Prejudice\" by Jane Austen from Project Gutenberg, tokenize the text, and display the first 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c158f3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\lalit\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens:\n",
      "['*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', '1342']\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import requests\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download text from Project Gutenberg using requests\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "raw_text = response.text\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(raw_text)\n",
    "print(\"First 10 tokens:\")\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afd861",
   "metadata": {},
   "source": [
    "2.\tUsing NLTK, write a function that takes a URL as input, fetches the raw text from the webpage, and returns the number of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc41fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens (including punctuation): 151046\n",
      "Unique words (alphabetic only): 6597\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def analyze_words_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    raw_text = response.text\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]  # keep only words\n",
    "    total_words = len(tokens)\n",
    "    unique_words = len(set(words))\n",
    "    return total_words, unique_words\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "total, unique = analyze_words_from_url(url)\n",
    "print(f\"Total tokens (including punctuation): {total}\")\n",
    "print(f\"Unique words (alphabetic only): {unique}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035cafd",
   "metadata": {},
   "source": [
    "3.\tExplain how to remove HTML tags from a web page's content using Python and NLTK. Provide a code example that fetches a web page, removes HTML tags, and prints the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aa3d44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lalit\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hugging Face · GitHub\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Navigation Menu\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Sign in\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "huggingface\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Product\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Copilot\n",
      "        Write better code with AI\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub Advanced Security\n",
      "        Find and fix vulnerabilities\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Actions\n",
      "        Automate any workflow\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Codespaces\n",
      "        Instant dev environments\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Issues\n",
      "        Plan and track work\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Code Review\n",
      "        Manage code changes\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discussions\n",
      "        Collaborate outside of code\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Code Search\n",
      "        Find more, search less\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Explore\n",
      "\n",
      "\n",
      "\n",
      "      All features\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "      Documentation\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      GitHub Skills\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Blog\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Solutions\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "By company size\n",
      "\n",
      "\n",
      "\n",
      "      Enterprises\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "      Small and medium tea\n",
      "First 10 tokens: ['Hugging', 'Face', '·', 'GitHub', 'Skip', 'to', 'content', 'Navigation', 'Menu', 'Toggle']\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def fetch_and_clean(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # Remove HTML tags\n",
    "    cleaned_text = soup.get_text()\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://github.com/huggingface\"\n",
    "clean_text = fetch_and_clean(url)\n",
    "print(\"Cleaned Text:\")\n",
    "print(clean_text[:1000])  # print first 1000 characters to avoid flooding the screen\n",
    "\n",
    "tokens = nltk.word_tokenize(clean_text)\n",
    "print(\"First 10 tokens:\", tokens[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec604aa",
   "metadata": {},
   "source": [
    "4.\tWrite a Python program that reads a text file, tokenizes its content into sentences, and prints the number of sentences in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "252d19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the file: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lalit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Download sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Open and read the file\n",
    "with open('para1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Output the number of sentences\n",
    "print(\"Number of sentences in the file:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add69bd",
   "metadata": {},
   "source": [
    "5.\tUsing regular expressions in Python, write a function that takes a list of words and returns a list of words that end with 'ing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e15649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ending with 'ing': ['running', 'swimming', 'coding']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def words_ending_with_ing(word_list):\n",
    "    pattern = re.compile(r'.*ing$')\n",
    "    return [word for word in word_list if pattern.match(word)]\n",
    "\n",
    "# Example usage:\n",
    "words = [\"running\", \"jog\", \"swimming\", \"play\", \"coding\", \"read\"]\n",
    "ing_words = words_ending_with_ing(words)\n",
    "print(\"Words ending with 'ing':\", ing_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
